{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtNBZFHO3M7n"
   },
   "source": [
    "# **TikTok Project**\n",
    "**Course 5 - Regression Analysis: Simplify complex data relationships**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gd59VyT2NhgK"
   },
   "source": [
    "You are a data professional at TikTok. The data team is working towards building a machine learning model that can be used to determine whether a video contains a claim or whether it offers an opinion. With a successful prediction model, TikTok can reduce the backlog of user reports and prioritize them more efficiently.\n",
    "\n",
    "The team is getting closer to completing the project, having completed an initial plan of action, initial Python coding work, EDA, and hypothesis testing.\n",
    "\n",
    "The TikTok team has reviewed the results of the hypothesis testing. TikTokâ€™s Operations Lead, Maika Abadi, is interested in how different variables are associated with whether a user is verified. Earlier, the data team observed that if a user is verified, they are much more likely to post opinions. Now, the data team has decided to explore how to predict verified status to help them understand how video characteristics relate to verified users. Therefore, you have been asked to conduct a logistic regression using verified status as the outcome variable. The results may be used to inform the final model related to predicting whether a video is a claim vs an opinion.\n",
    "\n",
    "A notebook was structured and prepared to help you in this project. Please complete the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgSbVJvomcVa"
   },
   "source": [
    "# **Course 5 End-of-course project: Regression modeling**\n",
    "\n",
    "\n",
    "In this activity, you will build a logistic regression model in Python. As you have learned, logistic regression helps you estimate the probability of an outcome. For data science professionals, this is a useful skill because it allows you to consider more than one variable against the variable you're measuring against. This opens the door for much more thorough and flexible analysis to be completed.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**The purpose** of this project is to demostrate knowledge of EDA and regression models.\n",
    "\n",
    "**The goal** is to build a logistic regression model and evaluate the model.\n",
    "<br/>\n",
    "*This activity has three parts:*\n",
    "\n",
    "**Part 1:** EDA & Checking Model Assumptions\n",
    "* What are some purposes of EDA before constructing a logistic regression model?\n",
    "\n",
    "**Part 2:** Model Building and Evaluation\n",
    "* What resources do you find yourself using as you complete this stage?\n",
    "\n",
    "**Part 3:** Interpreting Model Results\n",
    "\n",
    "* What key insights emerged from your model(s)?\n",
    "\n",
    "* What business recommendations do you propose based on the models built?\n",
    "\n",
    "Follow the instructions and answer the question below to complete the activity. Then, you will complete an executive summary using the questions listed on the PACE Strategy Document.\n",
    "\n",
    "Be sure to complete this activity before moving on. The next course item will provide you with a completed exemplar to compare to your own work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KFOyc3JPSiN"
   },
   "source": [
    "# **Build a regression model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UCHQclzQDUL"
   },
   "source": [
    "<img src=\"images/Pace.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "# **PACE stages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJfAWkcilRVx"
   },
   "source": [
    "Throughout these project notebooks, you'll see references to the problem-solving framework PACE. The following notebook components are labeled with the respective PACE stage: Plan, Analyze, Construct, and Execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5O5cx_qQJmX"
   },
   "source": [
    "<img src=\"images/Plan.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "\n",
    "## **PACE: Plan**\n",
    "Consider the questions in your PACE Strategy Document to reflect on the Plan stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8qYlvkLQsf2"
   },
   "source": [
    "### **Task 1. Imports and loading**\n",
    "Import the data and packages that you've learned are needed for building regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCni9wAGphb0"
   },
   "outputs": [],
   "source": [
    "# Import packages for data manipulation\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Import packages for data visualization\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Import packages for data preprocessing\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Import packages for data modeling\n",
    "### YOUR CODE HERE ###\n",
    "# Import packages for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import packages for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "#import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# Import packages for data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Import packages for data modeling\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjljvyG32kqe"
   },
   "source": [
    "Load the TikTok dataset.\n",
    "\n",
    "**Note:** As shown in this cell, the dataset has been automatically loaded in for you. You do not need to download the .csv file, or provide more code, in order to access the dataset and proceed with this lab. Please continue with this activity by completing the following instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9ODhaOppqlw"
   },
   "outputs": [],
   "source": [
    "# Load dataset into dataframe\n",
    "data = pd.read_csv(\"tiktok_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnrvCSfHUWPv"
   },
   "source": [
    "<img src=\"images/Analyze.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "## **PACE: Analyze**\n",
    "\n",
    "Consider the questions in your PACE Strategy Document to reflect on the Analyze stage.\n",
    "\n",
    "In this stage, consider the following question where applicable to complete your code response:\n",
    "\n",
    "* What are some purposes of EDA before constructing a logistic regression model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZA0zpCSpYIL"
   },
   "source": [
    "==> ENTER YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIcDG2e66wt9"
   },
   "source": [
    "### **Task 2a. Explore data with EDA**\n",
    "\n",
    "Analyze the data and check for and handle missing values and duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBHl90JIRuXk"
   },
   "source": [
    "Inspect the first five rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rfk95MLp4a_"
   },
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "### YOUR CODE HERE ###\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66KOxKCx977b"
   },
   "source": [
    "Get the number of rows and columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYwW-G1WqX3R"
   },
   "outputs": [],
   "source": [
    "# Get number of rows and columns\n",
    "### YOUR CODE HERE ###\n",
    "rows, columns = data.shape\n",
    "rows, columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP-NbEGdEio8"
   },
   "source": [
    "Get the data types of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbpDgrXfEoVj"
   },
   "outputs": [],
   "source": [
    "# Get data types of columns\n",
    "### YOUR CODE HERE ###\n",
    "data_types = data.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URuRVjUZ_Axg"
   },
   "source": [
    "Get basic information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyyKCGzCp7SS"
   },
   "outputs": [],
   "source": [
    "# Get basic information\n",
    "### YOUR CODE HERE ###\n",
    "data_info = data.info()\n",
    "data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T5Ieb6WB61Q"
   },
   "source": [
    "Generate basic descriptive statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbpuAS2UqY01"
   },
   "outputs": [],
   "source": [
    "# Generate basic descriptive stats\n",
    "descriptive_stats = data.describe(include='all')\n",
    "descriptive_stats\n",
    "descriptive_stats = data.describe(include='all')\n",
    "descriptive_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OpeNQDdyIT6"
   },
   "source": [
    "Check for and handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrunHcfa7xnT"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "### YOUR CODE HERE ###\n",
    "missing_values = data.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHSj1Hma914I"
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "### YOUR CODE HERE ###\n",
    "data_cleaned = data.dropna()\n",
    "\n",
    "# Verify the shape of the cleaned data\n",
    "data_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYUF3xap9_Fa"
   },
   "outputs": [],
   "source": [
    "# Display first few rows after handling missing values\n",
    "### YOUR CODE HERE ###\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcfffpANyNiu"
   },
   "source": [
    "Check for and handle duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKaGnWIsiHpH"
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "### YOUR CODE HERE ###\n",
    "duplicate_rows = data_cleaned.duplicated().sum()\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-HcHpGc5Hn7"
   },
   "source": [
    "Check for and handle outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ts_lcV6qUQHq"
   },
   "outputs": [],
   "source": [
    "# Create a boxplot to visualize distribution of `video_duration_sec`\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Create a boxplot to visualize the distribution of 'video_duration_sec'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(data_cleaned['video_duration_sec'])\n",
    "plt.title('Boxplot of Video Duration (Seconds)')\n",
    "plt.ylabel('Duration (Seconds)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlUccI91a4NH"
   },
   "outputs": [],
   "source": [
    "# Create a boxplot to visualize distribution of `video_view_count`\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Create a boxplot to visualize the distribution of 'video_view_count'\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(data_cleaned['video_view_count'])\n",
    "plt.title('Boxplot of Video View Count')\n",
    "plt.ylabel('View Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0UVdeRUzEEF"
   },
   "outputs": [],
   "source": [
    "# Create a boxplot to visualize distribution of `video_like_count`\n",
    "### YOUR CODE HERE ###\n",
    "# Create a boxplot to visualize the distribution of 'video_like_count'\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(data_cleaned['video_like_count'])\n",
    "plt.title('Boxplot of Video Like Count')\n",
    "plt.ylabel('Like Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gE1fkGo0eG9"
   },
   "outputs": [],
   "source": [
    "# Create a boxplot to visualize distribution of `video_comment_count`\n",
    "### YOUR CODE HERE ###\n",
    "# Create a boxplot to visualize the distribution of 'video_comment_count'\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(data_cleaned['video_comment_count'])\n",
    "plt.title('Boxplot of Video Comment Count')\n",
    "plt.ylabel('Comment Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85JfQprd5Kn9"
   },
   "outputs": [],
   "source": [
    "# Check for and handle outliers for video_like_count\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Q1 = data_cleaned['video_like_count'].quantile(0.25)\n",
    "Q3 = data_cleaned['video_like_count'].quantile(0.75)\n",
    "\n",
    "# Calculate the Interquartile Range (IQR)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the dataset to remove outliers\n",
    "data_no_outliers = data_cleaned[(data_cleaned['video_like_count'] >= lower_bound) & (data_cleaned['video_like_count'] <= upper_bound)]\n",
    "\n",
    "# Verify how many rows were removed\n",
    "outliers_removed = data_cleaned.shape[0] - data_no_outliers.shape[0]\n",
    "outliers_removed, data_no_outliers.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDcY0ZaEJgDl"
   },
   "source": [
    "Check class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3klRqlsJhtp"
   },
   "outputs": [],
   "source": [
    "# Check class balance for video_comment_count\n",
    "### YOUR CODE HERE ###\n",
    "# Check the distribution (class balance) for 'video_comment_count'\n",
    "comment_count_distribution = data_no_outliers['video_comment_count'].value_counts()\n",
    "\n",
    "comment_count_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePYWt2p5LqlW"
   },
   "source": [
    "Approximately 94.2% of the dataset represents videos posted by unverified accounts and 5.8% represents videos posted by verified accounts. So the outcome variable is not very balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRDMuXjbA9KQ"
   },
   "source": [
    "Use resampling to create class balance in the outcome variable, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWhetFT3Ggyp"
   },
   "outputs": [],
   "source": [
    "# Use resampling to create class balance in the outcome variable, if needed\n",
    "\n",
    "# Identify data points from majority and minority classes\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Upsample the minority class (which is \"verified\")\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Display new class counts\n",
    "### YOUR CODE HERE ###\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# First, simplify the class balance by grouping the 'video_comment_count' into ranges\n",
    "# (e.g., 0 comments, 1-10 comments, 11-100 comments, 101+ comments)\n",
    "data_no_outliers['comment_class'] = pd.cut(data_no_outliers['video_comment_count'],\n",
    "                                           bins=[-1, 0, 10, 100, data_no_outliers['video_comment_count'].max()],\n",
    "                                           labels=['0 comments', '1-10 comments', '11-100 comments', '101+ comments'])\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = data_no_outliers[data_no_outliers['comment_class'] == '0 comments']\n",
    "minority_classes = data_no_outliers[data_no_outliers['comment_class'] != '0 comments']\n",
    "\n",
    "# Upsample the minority classes\n",
    "minority_upsampled = resample(minority_classes,\n",
    "                              replace=True,   # Sample with replacement\n",
    "                              n_samples=len(majority_class),  # Match the majority class size\n",
    "                              random_state=42)\n",
    "\n",
    "# Combine the majority class with the upsampled minority classes\n",
    "balanced_data = pd.concat([majority_class, minority_upsampled])\n",
    "\n",
    "# Display new class counts for 'comment_class'\n",
    "new_class_counts = balanced_data['comment_class'].value_counts()\n",
    "new_class_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2RHONw6D3R6"
   },
   "source": [
    "Get the average `video_transcription_text` length for videos posted by verified accounts and the average `video_transcription_text` length for videos posted by unverified accounts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-aaFCPCD88e"
   },
   "outputs": [],
   "source": [
    "# Get the average `video_transcription_text` length for claims and the average `video_transcription_text` length for opinions\n",
    "### YOUR CODE HERE ###\n",
    "# First, calculate the length of each 'video_transcription_text'\n",
    "balanced_data['transcription_length'] = balanced_data['video_transcription_text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Now, calculate the average length for claims and opinions\n",
    "average_claim_length = balanced_data[balanced_data['claim_status'] == 'claim']['transcription_length'].mean()\n",
    "average_opinion_length = balanced_data[balanced_data['claim_status'] == 'opinion']['transcription_length'].mean()\n",
    "\n",
    "average_claim_length, average_opinion_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hb_u1c6_T1i-"
   },
   "source": [
    "Extract the length of each `video_transcription_text` and add this as a column to the dataframe, so that it can be used as a potential feature in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Yr9hhhVHeYY"
   },
   "outputs": [],
   "source": [
    "# Extract the length of each `video_transcription_text` and add this as a column to the dataframe\n",
    "### YOUR CODE HERE ###\n",
    "# Extract the length of each 'video_transcription_text' and add it as a column to the dataframe\n",
    "balanced_data['transcription_length'] = balanced_data['video_transcription_text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Display the first few rows to confirm the new column has been added\n",
    "balanced_data[['video_transcription_text', 'transcription_length']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vzg0J8UEJ1wx"
   },
   "outputs": [],
   "source": [
    "# Display first few rows of dataframe after adding new column\n",
    "### YOUR CODE HERE ###\n",
    "# Display the first few rows of the dataframe after adding the 'transcription_length' column\n",
    "balanced_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGu7ipi4AJmP"
   },
   "source": [
    "Visualize the distribution of `video_transcription_text` length for videos posted by verified accounts and videos posted by unverified accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSq136S3TIYe"
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of `video_transcription_text` length for videos posted by verified accounts and videos posted by unverified accounts\n",
    "# Create two histograms in one plot\n",
    "### YOUR CODE HERE ###\n",
    "# Filter the data based on verified and unverified accounts\n",
    "verified_accounts = balanced_data[balanced_data['verified_status'] == 'verified']\n",
    "unverified_accounts = balanced_data[balanced_data['verified_status'] == 'not verified']\n",
    "\n",
    "# Plot two histograms in one plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(verified_accounts['transcription_length'], bins=30, alpha=0.5, label='Verified Accounts', color='blue')\n",
    "plt.hist(unverified_accounts['transcription_length'], bins=30, alpha=0.5, label='Unverified Accounts', color='orange')\n",
    "\n",
    "plt.title('Distribution of Video Transcription Length for Verified and Unverified Accounts')\n",
    "plt.xlabel('Transcription Length (Characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlF7ZNSyW0yV"
   },
   "source": [
    "### **Task 2b. Examine correlations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKkx6FvS4OpI"
   },
   "source": [
    "Next, code a correlation matrix to help determine most correlated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCEzE-gwL5gq"
   },
   "outputs": [],
   "source": [
    "# Select only the numeric columns from the dataset\n",
    "numeric_columns = balanced_data.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Calculate the correlation matrix for the numeric columns\n",
    "correlation_matrix = numeric_columns.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "correlation_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ldmks6g4ZzE"
   },
   "source": [
    "Visualize a correlation heatmap of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ti_MFF7tekR"
   },
   "outputs": [],
   "source": [
    "# Create a heatmap to visualize how correlated variables are\n",
    "### YOUR CODE HERE ###\n",
    "# Create a heatmap to visualize the correlation matrix\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Generate a heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Correlation Heatmap of Numeric Variables')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyQgcrDI00bT"
   },
   "source": [
    "One of the model assumptions for logistic regression is no severe multicollinearity among the features. Take this into consideration as you examine the heatmap and choose which features to proceed with.\n",
    "\n",
    "**Question:** What variables are shown to be correlated in the heatmap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgPul2DiY6T4"
   },
   "source": [
    "<img src=\"images/Construct.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "## **PACE: Construct**\n",
    "\n",
    "After analysis and deriving variables with close relationships, it is time to begin constructing the model. Consider the questions in your PACE Strategy Document to reflect on the Construct stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07UJJm41ajgf"
   },
   "source": [
    "### **Task 3a. Select variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxCW0RH_4m3Q"
   },
   "source": [
    "Set your Y and X variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhocrccZBcwx"
   },
   "source": [
    "Select the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uH6jiQECBgFn"
   },
   "outputs": [],
   "source": [
    "# # Create a heatmap to visualize the correlation matrix\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Generate a heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Correlation Heatmap of Numeric Variables')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPxEovpUBZfk"
   },
   "source": [
    "Select the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJ5C6M5zCP3k"
   },
   "outputs": [],
   "source": [
    "# Select features\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Display first few rows of features dataframe\n",
    "### YOUR CODE HERE ###\n",
    "# Select relevant features\n",
    "features = balanced_data[['video_duration_sec', 'video_view_count', 'video_like_count', \n",
    "                          'video_share_count', 'video_download_count', 'video_comment_count', 'transcription_length']]\n",
    "\n",
    "# Display the first few rows of the selected features\n",
    "features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPIBz0-HDEqd"
   },
   "source": [
    "### **Task 3b. Train-test split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tNw9_y9jmY1"
   },
   "source": [
    "Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKgrew0V6o_3"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "### YOUR CODE HERE ###\n",
    "# Split the selected features into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the target variable (video_comment_count in this case) and features\n",
    "X = features.drop(columns=['video_comment_count'])\n",
    "y = features['video_comment_count']\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the training and testing sets\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VlklzoujrAR"
   },
   "source": [
    "Confirm that the dimensions of the training and testing sets are in alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgbB1NCtfxcc"
   },
   "outputs": [],
   "source": [
    "# Get shape of each training and testing set\n",
    "### YOUR CODE HERE ###\n",
    "# Display the shapes of the training and testing sets for both features and target\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7PjlXFuDRp_"
   },
   "source": [
    "### **Task 3c. Encode variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbpnEjop82zL"
   },
   "source": [
    "Check the data types of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TefBRXUu8zWR"
   },
   "outputs": [],
   "source": [
    "# Check data types\n",
    "### YOUR CODE HERE ###\n",
    "# Check the data types of the features in the training set\n",
    "X_train.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gT1YqFZc-uBF"
   },
   "outputs": [],
   "source": [
    "# Get unique values in `claim_status`\n",
    "### YOUR CODE HERE ###\n",
    "# Get the unique values in the 'claim_status' column\n",
    "unique_claim_status = balanced_data['claim_status'].unique()\n",
    "unique_claim_status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSnObsvy-wpz"
   },
   "outputs": [],
   "source": [
    "# Get unique values in `author_ban_status`\n",
    "### YOUR CODE HERE ###\n",
    "# Get the unique values in the 'author_ban_status' column\n",
    "unique_author_ban_status = balanced_data['author_ban_status'].unique()\n",
    "unique_author_ban_status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKSSWs39xYWH"
   },
   "source": [
    "As shown above, the `claim_status` and `author_ban_status` features are each of data type `object` currently. In order to work with the implementations of models through `sklearn`, these categorical features will need to be made numeric. One way to do this is through one-hot encoding.\n",
    "\n",
    "Encode categorical features in the training set using an appropriate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlMTSyaTcBct"
   },
   "outputs": [],
   "source": [
    "# Select the training features that needs to be encoded\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Display first few rows\n",
    "### YOUR CODE HERE ###\n",
    "# Select the categorical features that need to be encoded\n",
    "categorical_features = balanced_data[['claim_status', 'author_ban_status']]\n",
    "\n",
    "# Display the first few rows of the selected features\n",
    "categorical_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSM5lQioAjex"
   },
   "outputs": [],
   "source": [
    "# Set up an encoder for one-hot encoding the categorical features\n",
    "### YOUR CODE HERE ###\n",
    "# Correct the OneHotEncoder setup by using the 'sparse' parameter\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')  # drop='first' to avoid multicollinearity\n",
    "\n",
    "# Fit and transform the categorical features\n",
    "encoded_features = encoder.fit_transform(categorical_features)\n",
    "\n",
    "# Convert the result to a DataFrame and display the first few rows\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features.columns))\n",
    "encoded_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PH_KGRJApBM_"
   },
   "outputs": [],
   "source": [
    "# # Correct the OneHotEncoder setup by using the 'sparse' parameter\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')  # drop='first' to avoid multicollinearity\n",
    "\n",
    "# Fit and transform the categorical features\n",
    "encoded_features = encoder.fit_transform(categorical_features)\n",
    "\n",
    "# Convert the result to a DataFrame and display the first few rows\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features.columns))\n",
    "encoded_df.head()\n",
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j57gJjIFpyO_"
   },
   "outputs": [],
   "source": [
    "# Get feature names from encoder\n",
    "### YOUR CODE HERE ###\n",
    "# Get the feature names from the encoder\n",
    "encoded_feature_names = encoder.get_feature_names_out(categorical_features.columns)\n",
    "encoded_feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vSYk7n7phDI"
   },
   "outputs": [],
   "source": [
    "# Display first few rows of encoded training features\n",
    "### YOUR CODE HERE ###\n",
    "# Display the first few rows of the encoded training features (categorical features that have been one-hot encoded)\n",
    "encoded_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAn10kA9qcUa"
   },
   "outputs": [],
   "source": [
    "# Place encoded training features (which is currently an array) into a dataframe\n",
    "### YOUR CODE HERE ###\n",
    "# Convert the encoded array into a DataFrame\n",
    "encoded_train_df = pd.DataFrame(encoded_features, columns=encoded_feature_names)\n",
    "\n",
    "# Display the first few rows of the newly created DataFrame\n",
    "encoded_train_df.head()\n",
    "\n",
    "\n",
    "# Display first few rows\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pdBRVwnuwc0"
   },
   "outputs": [],
   "source": [
    "# Display first few rows of `X_train` with `claim_status` and `author_ban_status` columns dropped (since these features are being transformed to numeric)\n",
    "### YOUR CODE HERE ###\n",
    "# Display the first few rows of X_train with 'claim_status' and 'author_ban_status' columns dropped\n",
    "X_train_dropped = X_train.drop(columns=['claim_status', 'author_ban_status'], errors='ignore')\n",
    "\n",
    "# Display the first few rows\n",
    "X_train_dropped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKG1TK-KEfuB"
   },
   "outputs": [],
   "source": [
    "# Concatenate `X_train` and `X_train_encoded_df` to form the final dataframe for training data (`X_train_final`)\n",
    "# Note: Using `.reset_index(drop=True)` to reset the index in X_train after dropping `claim_status` and `author_ban_status`,\n",
    "# so that the indices align with those in `X_train_encoded_df` and `count_df`\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Display first few rows\n",
    "### YOUR CODE HERE ###\n",
    "# Concatenate X_train_dropped and the encoded features DataFrame (encoded_train_df)\n",
    "X_train_final = pd.concat([X_train_dropped.reset_index(drop=True), encoded_train_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Display the first few rows of the final training DataFrame\n",
    "X_train_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZHZcDR_fY-D"
   },
   "source": [
    "Check the data type of the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNu4ndvufeP2"
   },
   "outputs": [],
   "source": [
    "# Check data type of outcome variable\n",
    "### YOUR CODE HERE ###\n",
    "# Check the data type of the outcome variable (y_train)\n",
    "y_train.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23VVtIeD9fet"
   },
   "outputs": [],
   "source": [
    "# Get unique values of outcome variable\n",
    "### YOUR CODE HERE ###\n",
    "# Get the unique values of the outcome variable (y_train)\n",
    "unique_outcome_values = y_train.unique()\n",
    "unique_outcome_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBUb24WDyAiA"
   },
   "source": [
    "A shown above, the outcome variable is of data type `object` currently. One-hot encoding can be used to make this variable numeric.\n",
    "\n",
    "Encode categorical values of the outcome variable the training set using an appropriate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGeLvAbgIBGh"
   },
   "outputs": [],
   "source": [
    "# Set up an encoder for one-hot encoding the categorical outcome variable\n",
    "### YOUR CODE HERE ###\n",
    "### YOUR CODE HERE ###\n",
    "# Set up one-hot encoding for the outcome variable\n",
    "encoder_outcome = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Reshape y_train to a 2D array (required for OneHotEncoder)\n",
    "y_train_reshaped = y_train.values.reshape(-1, 1)\n",
    "\n",
    "# Fit and transform the outcome variable\n",
    "encoded_outcome = encoder_outcome.fit_transform(y_train_reshaped)\n",
    "\n",
    "# Convert the result to a DataFrame and display the first few rows\n",
    "encoded_outcome_df = pd.DataFrame(encoded_outcome, columns=encoder_outcome.get_feature_names_out(['video_comment_count']))\n",
    "encoded_outcome_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJKcy7sqGeSC"
   },
   "outputs": [],
   "source": [
    "# Encode the training outcome variable\n",
    "# Notes:\n",
    "#   - Adjusting the shape of `y_train` before passing into `.fit_transform()`, since it takes in 2D array\n",
    "#   - Using `.ravel()` to flatten the array returned by `.fit_transform()`, so that it can be used later to train the model\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Display the encoded training outcome variable\n",
    "### YOUR CODE HERE ###\n",
    "# Adjust the shape of y_train and perform one-hot encoding\n",
    "y_train_encoded = encoder_outcome.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Display the encoded training outcome variable\n",
    "y_train_encoded[:10]  # Display the first 10 entries of the encoded outcome variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDYyjWssbnBG"
   },
   "source": [
    "### **Task 3d. Model building**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty8ieBkDBH4g"
   },
   "source": [
    "Construct a model and fit it to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNcke2SNvZrN"
   },
   "outputs": [],
   "source": [
    "# Construct a logistic regression model and fit it to the training set\n",
    "### YOUR CODE HERE ###\n",
    "# Import the linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the linear regression model to the cleaned training set\n",
    "linear_model.fit(X_train_final_cleaned, y_train_cleaned)\n",
    "\n",
    "# Display a confirmation message\n",
    "\"Linear regression model has been successfully fitted to the cleaned training set.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp7ojoBldEYy"
   },
   "source": [
    "<img src=\"images/Execute.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "## **PACE: Execute**\n",
    "\n",
    "Consider the questions in your PACE Strategy Document to reflect on the Execute stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_l3bkxQdJ3a"
   },
   "source": [
    "### **Taks 4a. Results and evaluation**\n",
    "\n",
    "Evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxnKPq6dIUOB"
   },
   "source": [
    "Encode categorical features in the testing set using an appropriate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-bNI_2_Lp_2"
   },
   "outputs": [],
   "source": [
    "# Select the testing features that needs to be encoded\n",
    "### YOUR CODE HERE ###\n",
    "# Check the columns available in X_test\n",
    "X_test.columns\n",
    "\n",
    "\n",
    "# Display first few rows\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWa-7XD-Lp_3"
   },
   "outputs": [],
   "source": [
    "# Transform the testing features using the encoder\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Display first few rows of encoded testing features\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Drop rows with missing values in the testing features\n",
    "X_test_cleaned = X_test.dropna()\n",
    "\n",
    "# Display the first few rows of the cleaned testing set\n",
    "X_test_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nmuk2nAELp_3"
   },
   "outputs": [],
   "source": [
    "# Redefine X_train_final_numerical to contain only the numerical features from the training set\n",
    "X_train_final_numerical = X_train_dropped  # This contains only numerical features from the training set\n",
    "\n",
    "# Reinitialize the linear regression model\n",
    "linear_model_numerical = LinearRegression()\n",
    "\n",
    "# Fit the model with the numerical features in the training set\n",
    "linear_model_numerical.fit(X_train_final_numerical, y_train_cleaned)\n",
    "\n",
    "# Make predictions on the aligned test set\n",
    "predictions_final = linear_model_numerical.predict(X_test_final)\n",
    "\n",
    "# Recalculate the error metrics\n",
    "mae = mean_absolute_error(y_test_final, predictions_final)\n",
    "mse = mean_squared_error(y_test_final, predictions_final)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Display the error metrics\n",
    "mae, mse, rmse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPkMT-n17pV5"
   },
   "outputs": [],
   "source": [
    "# Display first few rows of `X_test` with `claim_status` and `author_ban_status` columns dropped (since these features are being transformed to numeric)\n",
    "### YOUR CODE HERE ###\n",
    "# Since 'claim_status' and 'author_ban_status' are already dropped from X_test, we can directly display the current X_test.\n",
    "# Display the first few rows of X_test (without 'claim_status' and 'author_ban_status')\n",
    "\n",
    "X_test_dropped = X_test.drop(columns=['claim_status', 'author_ban_status'], errors='ignore')\n",
    "\n",
    "# Display first few rows\n",
    "X_test_dropped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLRVCl2yLp_4"
   },
   "outputs": [],
   "source": [
    "# Concatenate `X_test` and `X_test_encoded_df` to form the final dataframe for training data (`X_test_final`)\n",
    "# Note: Using `.reset_index(drop=True)` to reset the index in X_test after dropping `claim_status`, and `author_ban_status`,\n",
    "# so that the indices align with those in `X_test_encoded_df` and `test_count_df`\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Display first few rows\n",
    "### YOUR CODE HERE ###\n",
    "# Reapply encoding to the categorical features in X_test (assuming they are not available in the current version)\n",
    "# Since we don't have 'claim_status' and 'author_ban_status' anymore, I will continue without them\n",
    "\n",
    "# Since there are no categorical features in the test set at this point, we can proceed with only the numerical features in X_test_dropped\n",
    "\n",
    "# Display the first few rows of X_test_dropped directly\n",
    "X_test_dropped.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM9egturW1eX"
   },
   "source": [
    "Test the logistic regression model. Use the model to make predictions on the encoded testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZQbthy93bWM"
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(data.drop(columns=['video_transcription_text', 'claim_status', 'author_ban_status'], errors='ignore'))\n",
    "\n",
    "# Define the target variable\n",
    "y = data['video_comment_count']\n",
    "\n",
    "# Drop rows with missing values in the feature set\n",
    "X_cleaned = X.dropna()\n",
    "y_cleaned = y[X_cleaned.index]\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model to the training data\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model (optional, depending on task)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display the first few predictions\n",
    "print('Predictions:', y_pred[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNjDzuqmYU0G"
   },
   "source": [
    "Display the predictions on the encoded testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyKjLA_gYUYZ"
   },
   "outputs": [],
   "source": [
    "# Display the predictions on the encoded testing set\n",
    "### YOUR CODE HERE ###\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Course_tiktok_dataset.csv')\n",
    "\n",
    "# Apply one-hot encoding to categorical features, drop unnecessary columns\n",
    "X = pd.get_dummies(data.drop(columns=['video_transcription_text', 'claim_status', 'author_ban_status'], errors='ignore'))\n",
    "\n",
    "# Define the target variable\n",
    "y = data['video_comment_count']\n",
    "\n",
    "# Drop rows with missing values in the feature set\n",
    "X_cleaned = X.dropna()\n",
    "y_cleaned = y[X_cleaned.index]\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model to the training data\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Display the first few predictions\n",
    "print('Predictions on the encoded testing set:', y_pred[:10])\n",
    "\n",
    "# Evaluate the model's accuracy (optional)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXDp4m9dYlN3"
   },
   "source": [
    "Display the true labels of the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JymZrHVDYdvu"
   },
   "outputs": [],
   "source": [
    "# Display the true labels of the testing set\n",
    "### YOUR CODE HERE ###\n",
    "X = pd.get_dummies(data.drop(columns=['video_transcription_text', 'claim_status', 'author_ban_status'], errors='ignore'))\n",
    "\n",
    "# Define the target variable\n",
    "y = data['video_comment_count']\n",
    "\n",
    "# Drop rows with missing values in the feature set\n",
    "X_cleaned = X.dropna()\n",
    "y_cleaned = y[X_cleaned.index]\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model to the training data\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Display the first few predictions and the true labels\n",
    "print('Predictions on the encoded testing set:', y_pred[:10])\n",
    "print('True labels of the testing set:', y_test[:10].values)\n",
    "\n",
    "# Evaluate the model's accuracy (optional)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WM-HHIInem3Z"
   },
   "source": [
    "Encode the true labels of the testing set so it can be compared to the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNnYVZnjfJfz"
   },
   "outputs": [],
   "source": [
    "# Encode the testing outcome variable\n",
    "# Notes:\n",
    "#   - Adjusting the shape of `y_test` before passing into `.transform()`, since it takes in 2D array\n",
    "#   - Using `.ravel()` to flatten the array returned by `.transform()`, so that it can be used later to compare with predictions\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Display the encoded testing outcome variable\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Reshape y_test to be a 2D array, as required by .transform()\n",
    "y_test_reshaped = y_test.values.reshape(-1, 1)\n",
    "\n",
    "# Fit and transform the outcome variable\n",
    "y_test_encoded = encoder.fit_transform(y_test_reshaped)\n",
    "\n",
    "# Flatten the array using .ravel()\n",
    "y_test_encoded_flat = y_test_encoded.ravel()\n",
    "\n",
    "# Display the encoded testing outcome variable\n",
    "print('Encoded testing outcome variable:', y_test_encoded_flat[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzdbI4A4mmHA"
   },
   "source": [
    "Confirm again that the dimensions of the training and testing sets are in alignment since additional features were added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbbI5cSfmmHA"
   },
   "outputs": [],
   "source": [
    "# Get shape of each training and testing set\n",
    "### YOUR CODE HERE ###\n",
    "# Display the shape of each set\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMQltTaQFnwv"
   },
   "source": [
    "### **Task 4b. Visualize model results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVMG0ubSXQvS"
   },
   "source": [
    "Create a confusion matrix to visualize the results of the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPgiHdlyZCTf"
   },
   "outputs": [],
   "source": [
    "# Compute values for confusion matrix\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Create display of confusion matrix\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Plot confusion matrix\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Display plot\n",
    "### YOUR CODE HERE ###\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a display of the confusion matrix using seaborn's heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6OmnATfbtNd"
   },
   "source": [
    "Create a classification report that includes precision, recall, f1-score, and accuracy metrics to evaluate the performance of the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMxCzRa-bnUe"
   },
   "outputs": [],
   "source": [
    "# Create a classification report\n",
    "### YOUR CODE HERE ###\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate the classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the classification report\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFY_y19EKQoO"
   },
   "source": [
    "### **Task 4c. Interpret model coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TmRk8tz4JpG"
   },
   "outputs": [],
   "source": [
    "# Get the feature names from the model and the model coefficients (which represent log-odds ratios)\n",
    "# Place into a DataFrame for readability\n",
    "### YOUR CODE HERE ###\n",
    "import pandas as pd\n",
    "\n",
    "# Extract feature names from the training set\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Get the model coefficients (log-odds ratios) from the logistic regression model\n",
    "coefficients = logistic_model.coef_[0]  # Extract the coefficients from the model\n",
    "\n",
    "# Create a DataFrame to display feature names alongside their coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient (Log-Odds Ratio)': coefficients\n",
    "})\n",
    "\n",
    "# Display the DataFrame sorted by the absolute value of coefficients\n",
    "coef_df_sorted = coef_df.sort_values(by='Coefficient (Log-Odds Ratio)', key=abs, ascending=False)\n",
    "print(coef_df_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6AlDDyhdzmG"
   },
   "source": [
    "### **Task 4d. Conclusion**\n",
    "\n",
    "1. What are the key takeaways from this project?\n",
    "\n",
    "2. What results can be presented from this project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzqJ13WAnt4R"
   },
   "source": [
    "==> ENTER YOUR RESPONSE TO QUESTIONS 1 AND 2 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** You've completed this lab. However, you may not notice a green check mark next to this item on Coursera's platform. Please continue your progress regardless of the check mark. Just click on the \"save\" icon at the top of this notebook to ensure your work has been logged. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
